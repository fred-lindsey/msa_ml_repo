{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - spacy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    catalogue-2.0.10           |  py311h267d04e_0          43 KB  conda-forge\n",
      "    cloudpathlib-0.19.0        |     pyhd8ed1ab_0          40 KB  conda-forge\n",
      "    confection-0.1.4           |  py311h8d5925d_0          88 KB  conda-forge\n",
      "    cymem-2.0.8                |  py311ha891d26_1          45 KB  conda-forge\n",
      "    cython-blis-0.7.10         |  py311hb49d859_2         610 KB  conda-forge\n",
      "    langcodes-3.3.0            |     pyhd8ed1ab_0         156 KB  conda-forge\n",
      "    murmurhash-1.0.10          |  py311ha891d26_1          31 KB  conda-forge\n",
      "    preshed-3.0.9              |  py311ha891d26_1         101 KB  conda-forge\n",
      "    shellingham-1.5.4          |     pyhd8ed1ab_0          14 KB  conda-forge\n",
      "    spacy-3.7.6                |  py311hfb6fb7d_0         5.0 MB  conda-forge\n",
      "    spacy-legacy-3.0.12        |     pyhd8ed1ab_0          28 KB  conda-forge\n",
      "    spacy-loggers-1.0.5        |     pyhd8ed1ab_0          21 KB  conda-forge\n",
      "    srsly-2.4.8                |  py311ha891d26_1         666 KB  conda-forge\n",
      "    thinc-8.2.5                |  py311hd6dc194_0         847 KB  conda-forge\n",
      "    typer-0.9.4                |     pyhd8ed1ab_0          78 KB  conda-forge\n",
      "    wasabi-1.1.2               |  py311h267d04e_1          58 KB  conda-forge\n",
      "    weasel-0.4.0               |     pyhd8ed1ab_0          42 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         7.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  catalogue          conda-forge/osx-arm64::catalogue-2.0.10-py311h267d04e_0 \n",
      "  cloudpathlib       conda-forge/noarch::cloudpathlib-0.19.0-pyhd8ed1ab_0 \n",
      "  confection         conda-forge/osx-arm64::confection-0.1.4-py311h8d5925d_0 \n",
      "  cymem              conda-forge/osx-arm64::cymem-2.0.8-py311ha891d26_1 \n",
      "  cython-blis        conda-forge/osx-arm64::cython-blis-0.7.10-py311hb49d859_2 \n",
      "  langcodes          conda-forge/noarch::langcodes-3.3.0-pyhd8ed1ab_0 \n",
      "  murmurhash         conda-forge/osx-arm64::murmurhash-1.0.10-py311ha891d26_1 \n",
      "  preshed            conda-forge/osx-arm64::preshed-3.0.9-py311ha891d26_1 \n",
      "  shellingham        conda-forge/noarch::shellingham-1.5.4-pyhd8ed1ab_0 \n",
      "  spacy              conda-forge/osx-arm64::spacy-3.7.6-py311hfb6fb7d_0 \n",
      "  spacy-legacy       conda-forge/noarch::spacy-legacy-3.0.12-pyhd8ed1ab_0 \n",
      "  spacy-loggers      conda-forge/noarch::spacy-loggers-1.0.5-pyhd8ed1ab_0 \n",
      "  srsly              conda-forge/osx-arm64::srsly-2.4.8-py311ha891d26_1 \n",
      "  thinc              conda-forge/osx-arm64::thinc-8.2.5-py311hd6dc194_0 \n",
      "  typer              conda-forge/noarch::typer-0.9.4-pyhd8ed1ab_0 \n",
      "  wasabi             conda-forge/osx-arm64::wasabi-1.1.2-py311h267d04e_1 \n",
      "  weasel             conda-forge/noarch::weasel-0.4.0-pyhd8ed1ab_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/osx-arm64::certifi-2024.8.3~ --> conda-forge/noarch::certifi-2024.8.30-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "spacy-3.7.6          | 5.0 MB    |                                       |   0% \n",
      "thinc-8.2.5          | 847 KB    |                                       |   0% \u001b[A\n",
      "\n",
      "srsly-2.4.8          | 666 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "cython-blis-0.7.10   | 610 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "langcodes-3.3.0      | 156 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "preshed-3.0.9        | 101 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "confection-0.1.4     | 88 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "typer-0.9.4          | 78 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "wasabi-1.1.2         | 58 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cymem-2.0.8          | 45 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "catalogue-2.0.10     | 43 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "weasel-0.4.0         | 42 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cloudpathlib-0.19.0  | 40 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "murmurhash-1.0.10    | 31 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "spacy-legacy-3.0.12  | 28 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "spacy-loggers-1.0.5  | 21 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "shellingham-1.5.4    | 14 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "cython-blis-0.7.10   | 610 KB    | 9                                     |   3% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "spacy-3.7.6          | 5.0 MB    | 1                                     |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "langcodes-3.3.0      | 156 KB    | ###7                                  |  10% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "langcodes-3.3.0      | 156 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "thinc-8.2.5          | 847 KB    | 6                                     |   2% \u001b[A\n",
      "\n",
      "srsly-2.4.8          | 666 KB    | ########8                             |  24% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "spacy-3.7.6          | 5.0 MB    | ##                                    |   6% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "cython-blis-0.7.10   | 610 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "thinc-8.2.5          | 847 KB    | ###################5                  |  53% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "preshed-3.0.9        | 101 KB    | #####8                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "srsly-2.4.8          | 666 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "srsly-2.4.8          | 666 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "spacy-3.7.6          | 5.0 MB    | #######5                              |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "confection-0.1.4     | 88 KB     | ######7                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "typer-0.9.4          | 78 KB     | #######5                              |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "spacy-3.7.6          | 5.0 MB    | ##############3                       |  39% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "wasabi-1.1.2         | 58 KB     | ##########2                           |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "wasabi-1.1.2         | 58 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "thinc-8.2.5          | 847 KB    | ##################################### | 100% \u001b[A\n",
      "thinc-8.2.5          | 847 KB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "catalogue-2.0.10     | 43 KB     | #############7                        |  37% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cymem-2.0.8          | 45 KB     | #############2                        |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "spacy-3.7.6          | 5.0 MB    | #####################4                |  58% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "typer-0.9.4          | 78 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "typer-0.9.4          | 78 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "catalogue-2.0.10     | 43 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cymem-2.0.8          | 45 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "weasel-0.4.0         | 42 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "spacy-3.7.6          | 5.0 MB    | #############################8        |  81% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cloudpathlib-0.19.0  | 40 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "murmurhash-1.0.10    | 31 KB     | ###################                   |  51% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "murmurhash-1.0.10    | 31 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "spacy-loggers-1.0.5  | 21 KB     | ###########################8          |  75% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "spacy-legacy-3.0.12  | 28 KB     | #####################1                |  57% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "spacy-loggers-1.0.5  | 21 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "spacy-legacy-3.0.12  | 28 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "shellingham-1.5.4    | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load( 'en_core_web_sm' )\n",
    "doc = nlp( \"This is a sentence.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "October 23 :  DATE\n",
      "Apple :  ORG\n",
      "Tim Cook :  PERSON\n",
      "iPad :  ORG\n",
      "fourth :  ORDINAL\n",
      "iPad :  ORG\n",
      "Retina :  PERSON\n",
      "iMac :  ORG\n",
      "13-inch :  QUANTITY\n",
      "MacBook Pro :  PERSON\n",
      "Retina :  PERSON\n"
     ]
    }
   ],
   "source": [
    "txt = 'On October 23, Apple CEO Tim Cook Unveiled the new iPad Mini, fourth generation iPad with Retina display, new iMac, and the 13-inch MacBook Pro with Retina display.'\n",
    "doc = nlp( txt )\n",
    "for NE in doc.ents:\n",
    "    print( NE.text, ': ', NE.label_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problem #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text is an excerpt from 'Of Mice and Men'\n",
    "\n",
    "text = \"\"\" Two men, dressed in denim jackets and trousers and wearing \"black, shapeless hats,\" walk single-file down a path near the pool. Both men carry blanket rolls — called bindles — on their shoulders. The smaller, wiry man is George Milton. Behind him is Lennie Small, a huge man with large eyes and sloping shoulders, walking at a gait that makes him resemble a huge bear.\n",
    "\n",
    "When Lennie drops near the pool's edge and begins to drink like a hungry animal, George cautions him that the water may not be good. This advice is necessary because Lennie is retarded and doesn't realize the possible dangers. The two are on their way to a ranch where they can get temporary work, and George warns Lennie not to say anything when they arrive. Because Lennie forgets things very quickly, George must make him repeat even the simplest instructions.\n",
    "\n",
    "Lennie also likes to pet soft things. In his pocket, he has a dead mouse which George confiscates and throws into the weeds beyond the pond. Lennie retrieves the dead mouse, and George once again catches him and gives Lennie a lecture about the trouble he causes when he wants to pet soft things (they were run out of the last town because Lennie touched a girl's soft dress, and she screamed). Lennie offers to leave and go live in a cave, causing George to soften his complaint and tell Lennie perhaps they can get him a puppy that can withstand Lennie's petting.\n",
    "\n",
    "As they get ready to eat and sleep for the night, Lennie asks George to repeat their dream of having their own ranch where Lennie will be able to tend rabbits. George does so and then warns Lennie that, if anything bad happens, Lennie is to come back to this spot and hide in the brush. Before George falls asleep, Lennie tells him they must have many rabbits of various colors.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem question: \n",
    "\n",
    "Convert this text into four separate text representations:\n",
    "\n",
    "- character\n",
    "- term\n",
    "- bigram, pairs of adjacent terms\n",
    "- term with appropriate part-of-speech disambiguation\n",
    "\n",
    "\n",
    "You can use Python's nltk or spaCy libraries to assign part-of-speech tags to terms in a term list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characater Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NLTK\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')  # Tokenizer for splitting text into words and sentences\n",
    "nltk.download('stopwords')  # Common stopwords like 'the', 'is', etc.\n",
    "nltk.download('averaged_perceptron_tagger')  # Part-of-speech tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'T', 'w', 'o', ' ', 'm', 'e', 'n', ',', ' ', 'd', 'r', 'e', 's', 's', 'e', 'd', ' ', 'i', 'n', ' ', 'd', 'e', 'n', 'i', 'm', ' ', 'j', 'a', 'c', 'k', 'e', 't', 's', ' ', 'a', 'n', 'd', ' ', 't', 'r', 'o', 'u', 's', 'e', 'r', 's', ' ', 'a', 'n', 'd', ' ', 'w', 'e', 'a', 'r', 'i', 'n', 'g', ' ', '\"', 'b', 'l', 'a', 'c', 'k', ',', ' ', 's', 'h', 'a', 'p', 'e', 'l', 'e', 's', 's', ' ', 'h', 'a', 't', 's', ',', '\"', ' ', 'w', 'a', 'l', 'k', ' ', 's', 'i', 'n', 'g', 'l', 'e', '-', 'f', 'i', 'l', 'e', ' ', 'd', 'o', 'w', 'n', ' ', 'a', ' ', 'p', 'a', 't', 'h', ' ', 'n', 'e', 'a', 'r', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'o', 'l', '.', ' ', 'B', 'o', 't', 'h', ' ', 'm', 'e', 'n', ' ', 'c', 'a', 'r', 'r', 'y', ' ', 'b', 'l', 'a', 'n', 'k', 'e', 't', ' ', 'r', 'o', 'l', 'l', 's', ' ', '—', ' ', 'c', 'a', 'l', 'l', 'e', 'd', ' ', 'b', 'i', 'n', 'd', 'l', 'e', 's', ' ', '—', ' ', 'o', 'n', ' ', 't', 'h', 'e', 'i', 'r', ' ', 's', 'h', 'o', 'u', 'l', 'd', 'e', 'r', 's', '.', ' ', 'T', 'h', 'e', ' ', 's', 'm', 'a', 'l', 'l', 'e', 'r', ',', ' ', 'w', 'i', 'r', 'y', ' ', 'm', 'a', 'n', ' ', 'i', 's', ' ', 'G', 'e', 'o', 'r', 'g', 'e', ' ', 'M', 'i', 'l', 't', 'o', 'n', '.', ' ', 'B', 'e', 'h', 'i', 'n', 'd', ' ', 'h', 'i', 'm', ' ', 'i', 's', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'S', 'm', 'a', 'l', 'l', ',', ' ', 'a', ' ', 'h', 'u', 'g', 'e', ' ', 'm', 'a', 'n', ' ', 'w', 'i', 't', 'h', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 'e', 'y', 'e', 's', ' ', 'a', 'n', 'd', ' ', 's', 'l', 'o', 'p', 'i', 'n', 'g', ' ', 's', 'h', 'o', 'u', 'l', 'd', 'e', 'r', 's', ',', ' ', 'w', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'a', 't', ' ', 'a', ' ', 'g', 'a', 'i', 't', ' ', 't', 'h', 'a', 't', ' ', 'm', 'a', 'k', 'e', 's', ' ', 'h', 'i', 'm', ' ', 'r', 'e', 's', 'e', 'm', 'b', 'l', 'e', ' ', 'a', ' ', 'h', 'u', 'g', 'e', ' ', 'b', 'e', 'a', 'r', '.', '\\n', '\\n', 'W', 'h', 'e', 'n', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'd', 'r', 'o', 'p', 's', ' ', 'n', 'e', 'a', 'r', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'o', 'l', \"'\", 's', ' ', 'e', 'd', 'g', 'e', ' ', 'a', 'n', 'd', ' ', 'b', 'e', 'g', 'i', 'n', 's', ' ', 't', 'o', ' ', 'd', 'r', 'i', 'n', 'k', ' ', 'l', 'i', 'k', 'e', ' ', 'a', ' ', 'h', 'u', 'n', 'g', 'r', 'y', ' ', 'a', 'n', 'i', 'm', 'a', 'l', ',', ' ', 'G', 'e', 'o', 'r', 'g', 'e', ' ', 'c', 'a', 'u', 't', 'i', 'o', 'n', 's', ' ', 'h', 'i', 'm', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'w', 'a', 't', 'e', 'r', ' ', 'm', 'a', 'y', ' ', 'n', 'o', 't', ' ', 'b', 'e', ' ', 'g', 'o', 'o', 'd', '.', ' ', 'T', 'h', 'i', 's', ' ', 'a', 'd', 'v', 'i', 'c', 'e', ' ', 'i', 's', ' ', 'n', 'e', 'c', 'e', 's', 's', 'a', 'r', 'y', ' ', 'b', 'e', 'c', 'a', 'u', 's', 'e', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'i', 's', ' ', 'r', 'e', 't', 'a', 'r', 'd', 'e', 'd', ' ', 'a', 'n', 'd', ' ', 'd', 'o', 'e', 's', 'n', \"'\", 't', ' ', 'r', 'e', 'a', 'l', 'i', 'z', 'e', ' ', 't', 'h', 'e', ' ', 'p', 'o', 's', 's', 'i', 'b', 'l', 'e', ' ', 'd', 'a', 'n', 'g', 'e', 'r', 's', '.', ' ', 'T', 'h', 'e', ' ', 't', 'w', 'o', ' ', 'a', 'r', 'e', ' ', 'o', 'n', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'w', 'a', 'y', ' ', 't', 'o', ' ', 'a', ' ', 'r', 'a', 'n', 'c', 'h', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 't', 'h', 'e', 'y', ' ', 'c', 'a', 'n', ' ', 'g', 'e', 't', ' ', 't', 'e', 'm', 'p', 'o', 'r', 'a', 'r', 'y', ' ', 'w', 'o', 'r', 'k', ',', ' ', 'a', 'n', 'd', ' ', 'G', 'e', 'o', 'r', 'g', 'e', ' ', 'w', 'a', 'r', 'n', 's', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'n', 'o', 't', ' ', 't', 'o', ' ', 's', 'a', 'y', ' ', 'a', 'n', 'y', 't', 'h', 'i', 'n', 'g', ' ', 'w', 'h', 'e', 'n', ' ', 't', 'h', 'e', 'y', ' ', 'a', 'r', 'r', 'i', 'v', 'e', '.', ' ', 'B', 'e', 'c', 'a', 'u', 's', 'e', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'f', 'o', 'r', 'g', 'e', 't', 's', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ', 'v', 'e', 'r', 'y', ' ', 'q', 'u', 'i', 'c', 'k', 'l', 'y', ',', ' ', 'G', 'e', 'o', 'r', 'g', 'e', ' ', 'm', 'u', 's', 't', ' ', 'm', 'a', 'k', 'e', ' ', 'h', 'i', 'm', ' ', 'r', 'e', 'p', 'e', 'a', 't', ' ', 'e', 'v', 'e', 'n', ' ', 't', 'h', 'e', ' ', 's', 'i', 'm', 'p', 'l', 'e', 's', 't', ' ', 'i', 'n', 's', 't', 'r', 'u', 'c', 't', 'i', 'o', 'n', 's', '.', '\\n', '\\n', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'a', 'l', 's', 'o', ' ', 'l', 'i', 'k', 'e', 's', ' ', 't', 'o', ' ', 'p', 'e', 't', ' ', 's', 'o', 'f', 't', ' ', 't', 'h', 'i', 'n', 'g', 's', '.', ' ', 'I', 'n', ' ', 'h', 'i', 's', ' ', 'p', 'o', 'c', 'k', 'e', 't', ',', ' ', 'h', 'e', ' ', 'h', 'a', 's', ' ', 'a', ' ', 'd', 'e', 'a', 'd', ' ', 'm', 'o', 'u', 's', 'e', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'G', 'e', 'o', 'r', 'g', 'e', ' ', 'c', 'o', 'n', 'f', 'i', 's', 'c', 'a', 't', 'e', 's', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'r', 'o', 'w', 's', ' ', 'i', 'n', 't', 'o', ' ', 't', 'h', 'e', ' ', 'w', 'e', 'e', 'd', 's', ' ', 'b', 'e', 'y', 'o', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'p', 'o', 'n', 'd', '.', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'r', 'e', 't', 'r', 'i', 'e', 'v', 'e', 's', ' ', 't', 'h', 'e', ' ', 'd', 'e', 'a', 'd', ' ', 'm', 'o', 'u', 's', 'e', ',', ' ', 'a', 'n', 'd', ' ', 'G', 'e', 'o', 'r', 'g', 'e', ' ', 'o', 'n', 'c', 'e', ' ', 'a', 'g', 'a', 'i', 'n', ' ', 'c', 'a', 't', 'c', 'h', 'e', 's', ' ', 'h', 'i', 'm', ' ', 'a', 'n', 'd', ' ', 'g', 'i', 'v', 'e', 's', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'a', ' ', 'l', 'e', 'c', 't', 'u', 'r', 'e', ' ', 'a', 'b', 'o', 'u', 't', ' ', 't', 'h', 'e', ' ', 't', 'r', 'o', 'u', 'b', 'l', 'e', ' ', 'h', 'e', ' ', 'c', 'a', 'u', 's', 'e', 's', ' ', 'w', 'h', 'e', 'n', ' ', 'h', 'e', ' ', 'w', 'a', 'n', 't', 's', ' ', 't', 'o', ' ', 'p', 'e', 't', ' ', 's', 'o', 'f', 't', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ', '(', 't', 'h', 'e', 'y', ' ', 'w', 'e', 'r', 'e', ' ', 'r', 'u', 'n', ' ', 'o', 'u', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'l', 'a', 's', 't', ' ', 't', 'o', 'w', 'n', ' ', 'b', 'e', 'c', 'a', 'u', 's', 'e', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 't', 'o', 'u', 'c', 'h', 'e', 'd', ' ', 'a', ' ', 'g', 'i', 'r', 'l', \"'\", 's', ' ', 's', 'o', 'f', 't', ' ', 'd', 'r', 'e', 's', 's', ',', ' ', 'a', 'n', 'd', ' ', 's', 'h', 'e', ' ', 's', 'c', 'r', 'e', 'a', 'm', 'e', 'd', ')', '.', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'o', 'f', 'f', 'e', 'r', 's', ' ', 't', 'o', ' ', 'l', 'e', 'a', 'v', 'e', ' ', 'a', 'n', 'd', ' ', 'g', 'o', ' ', 'l', 'i', 'v', 'e', ' ', 'i', 'n', ' ', 'a', ' ', 'c', 'a', 'v', 'e', ',', ' ', 'c', 'a', 'u', 's', 'i', 'n', 'g', ' ', 'G', 'e', 'o', 'r', 'g', 'e', ' ', 't', 'o', ' ', 's', 'o', 'f', 't', 'e', 'n', ' ', 'h', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'l', 'a', 'i', 'n', 't', ' ', 'a', 'n', 'd', ' ', 't', 'e', 'l', 'l', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'p', 'e', 'r', 'h', 'a', 'p', 's', ' ', 't', 'h', 'e', 'y', ' ', 'c', 'a', 'n', ' ', 'g', 'e', 't', ' ', 'h', 'i', 'm', ' ', 'a', ' ', 'p', 'u', 'p', 'p', 'y', ' ', 't', 'h', 'a', 't', ' ', 'c', 'a', 'n', ' ', 'w', 'i', 't', 'h', 's', 't', 'a', 'n', 'd', ' ', 'L', 'e', 'n', 'n', 'i', 'e', \"'\", 's', ' ', 'p', 'e', 't', 't', 'i', 'n', 'g', '.', '\\n', '\\n', 'A', 's', ' ', 't', 'h', 'e', 'y', ' ', 'g', 'e', 't', ' ', 'r', 'e', 'a', 'd', 'y', ' ', 't', 'o', ' ', 'e', 'a', 't', ' ', 'a', 'n', 'd', ' ', 's', 'l', 'e', 'e', 'p', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'n', 'i', 'g', 'h', 't', ',', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'a', 's', 'k', 's', ' ', 'G', 'e', 'o', 'r', 'g', 'e', ' ', 't', 'o', ' ', 'r', 'e', 'p', 'e', 'a', 't', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'd', 'r', 'e', 'a', 'm', ' ', 'o', 'f', ' ', 'h', 'a', 'v', 'i', 'n', 'g', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'o', 'w', 'n', ' ', 'r', 'a', 'n', 'c', 'h', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'w', 'i', 'l', 'l', ' ', 'b', 'e', ' ', 'a', 'b', 'l', 'e', ' ', 't', 'o', ' ', 't', 'e', 'n', 'd', ' ', 'r', 'a', 'b', 'b', 'i', 't', 's', '.', ' ', 'G', 'e', 'o', 'r', 'g', 'e', ' ', 'd', 'o', 'e', 's', ' ', 's', 'o', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 'w', 'a', 'r', 'n', 's', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 't', 'h', 'a', 't', ',', ' ', 'i', 'f', ' ', 'a', 'n', 'y', 't', 'h', 'i', 'n', 'g', ' ', 'b', 'a', 'd', ' ', 'h', 'a', 'p', 'p', 'e', 'n', 's', ',', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 'i', 's', ' ', 't', 'o', ' ', 'c', 'o', 'm', 'e', ' ', 'b', 'a', 'c', 'k', ' ', 't', 'o', ' ', 't', 'h', 'i', 's', ' ', 's', 'p', 'o', 't', ' ', 'a', 'n', 'd', ' ', 'h', 'i', 'd', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'b', 'r', 'u', 's', 'h', '.', ' ', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'G', 'e', 'o', 'r', 'g', 'e', ' ', 'f', 'a', 'l', 'l', 's', ' ', 'a', 's', 'l', 'e', 'e', 'p', ',', ' ', 'L', 'e', 'n', 'n', 'i', 'e', ' ', 't', 'e', 'l', 'l', 's', ' ', 'h', 'i', 'm', ' ', 't', 'h', 'e', 'y', ' ', 'm', 'u', 's', 't', ' ', 'h', 'a', 'v', 'e', ' ', 'm', 'a', 'n', 'y', ' ', 'r', 'a', 'b', 'b', 'i', 't', 's', ' ', 'o', 'f', ' ', 'v', 'a', 'r', 'i', 'o', 'u', 's', ' ', 'c', 'o', 'l', 'o', 'r', 's', '.']\n"
     ]
    }
   ],
   "source": [
    "# character level tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "char_tokenizer = RegexpTokenizer(r'.', gaps=False)\n",
    "\n",
    "# call the function on the text block\n",
    "char_tokens = char_tokenizer.tokenize(text)\n",
    "\n",
    "print(char_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 316, 'T': 4, 'w': 26, 'o': 85, 'm': 30, 'e': 214, 'n': 122, ',': 17, 'd': 50, 'r': 77, 's': 102, 'i': 91, 'j': 1, 'a': 117, 'c': 34, 'k': 15, 't': 111, 'u': 27, 'g': 40, '\"': 2, 'b': 22, 'l': 51, 'h': 81, 'p': 27, '-': 1, 'f': 16, '.': 15, 'B': 4, 'y': 23, '—': 2, 'G': 10, 'M': 1, 'L': 17, 'S': 1, '\\n': 6, 'W': 1, \"'\": 4, 'v': 12, 'z': 1, 'q': 1, 'I': 1, '(': 1, ')': 1, 'A': 1}\n"
     ]
    }
   ],
   "source": [
    "char_count = {}\n",
    "\n",
    "for char in char_tokens:\n",
    "    if char not in char_count:\n",
    "        char_count[char] = 0\n",
    "    char_count[char] +=1\n",
    "\n",
    "print(char_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Char</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>o</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>e</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>r</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>s</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>j</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>a</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>c</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>k</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>t</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>u</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>g</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>b</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>l</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>h</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>p</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>f</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>.</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>y</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>—</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>G</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>L</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>\\n</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>W</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>'</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>v</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>q</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>I</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Char  Count\n",
       "0          316\n",
       "1     T      4\n",
       "2     w     26\n",
       "3     o     85\n",
       "4     m     30\n",
       "5     e    214\n",
       "6     n    122\n",
       "7     ,     17\n",
       "8     d     50\n",
       "9     r     77\n",
       "10    s    102\n",
       "11    i     91\n",
       "12    j      1\n",
       "13    a    117\n",
       "14    c     34\n",
       "15    k     15\n",
       "16    t    111\n",
       "17    u     27\n",
       "18    g     40\n",
       "19    \"      2\n",
       "20    b     22\n",
       "21    l     51\n",
       "22    h     81\n",
       "23    p     27\n",
       "24    -      1\n",
       "25    f     16\n",
       "26    .     15\n",
       "27    B      4\n",
       "28    y     23\n",
       "29    —      2\n",
       "30    G     10\n",
       "31    M      1\n",
       "32    L     17\n",
       "33    S      1\n",
       "34   \\n      6\n",
       "35    W      1\n",
       "36    '      4\n",
       "37    v     12\n",
       "38    z      1\n",
       "39    q      1\n",
       "40    I      1\n",
       "41    (      1\n",
       "42    )      1\n",
       "43    A      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_count_df = pd.DataFrame(list(char_count.items()), columns = ['Char', 'Count'] )\n",
    "char_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Two': 1, 'men': 2, ',': 17, 'dressed': 1, 'in': 3, 'denim': 1, 'jackets': 1, 'and': 15, 'trousers': 1, 'wearing': 1, '``': 1, 'black': 1, 'shapeless': 1, 'hats': 1, \"''\": 1, 'walk': 1, 'single-file': 1, 'down': 1, 'a': 11, 'path': 1, 'near': 2, 'the': 12, 'pool': 2, '.': 15, 'Both': 1, 'carry': 1, 'blanket': 1, 'rolls': 1, '—': 2, 'called': 1, 'bindles': 1, 'on': 2, 'their': 4, 'shoulders': 2, 'The': 2, 'smaller': 1, 'wiry': 1, 'man': 2, 'is': 5, 'George': 10, 'Milton': 1, 'Behind': 1, 'him': 7, 'Lennie': 17, 'Small': 1, 'huge': 2, 'with': 1, 'large': 1, 'eyes': 1, 'sloping': 1, 'walking': 1, 'at': 1, 'gait': 1, 'that': 4, 'makes': 1, 'resemble': 1, 'bear': 1, 'When': 1, 'drops': 1, \"'s\": 3, 'edge': 1, 'begins': 1, 'to': 12, 'drink': 1, 'like': 1, 'hungry': 1, 'animal': 1, 'cautions': 1, 'water': 1, 'may': 1, 'not': 2, 'be': 2, 'good': 1, 'This': 1, 'advice': 1, 'necessary': 1, 'because': 2, 'retarded': 1, 'does': 2, \"n't\": 1, 'realize': 1, 'possible': 1, 'dangers': 1, 'two': 1, 'are': 1, 'way': 1, 'ranch': 2, 'where': 2, 'they': 6, 'can': 3, 'get': 3, 'temporary': 1, 'work': 1, 'warns': 2, 'say': 1, 'anything': 2, 'when': 2, 'arrive': 1, 'Because': 1, 'forgets': 1, 'things': 3, 'very': 1, 'quickly': 1, 'must': 2, 'make': 1, 'repeat': 2, 'even': 1, 'simplest': 1, 'instructions': 1, 'also': 1, 'likes': 1, 'pet': 2, 'soft': 3, 'In': 1, 'his': 2, 'pocket': 1, 'he': 3, 'has': 1, 'dead': 2, 'mouse': 2, 'which': 1, 'confiscates': 1, 'throws': 1, 'into': 1, 'weeds': 1, 'beyond': 1, 'pond': 1, 'retrieves': 1, 'once': 1, 'again': 1, 'catches': 1, 'gives': 1, 'lecture': 1, 'about': 1, 'trouble': 1, 'causes': 1, 'wants': 1, '(': 1, 'were': 1, 'run': 1, 'out': 1, 'of': 3, 'last': 1, 'town': 1, 'touched': 1, 'girl': 1, 'dress': 1, 'she': 1, 'screamed': 1, ')': 1, 'offers': 1, 'leave': 1, 'go': 1, 'live': 1, 'cave': 1, 'causing': 1, 'soften': 1, 'complaint': 1, 'tell': 1, 'perhaps': 1, 'puppy': 1, 'withstand': 1, 'petting': 1, 'As': 1, 'ready': 1, 'eat': 1, 'sleep': 1, 'for': 1, 'night': 1, 'asks': 1, 'dream': 1, 'having': 1, 'own': 1, 'will': 1, 'able': 1, 'tend': 1, 'rabbits': 2, 'so': 1, 'then': 1, 'if': 1, 'bad': 1, 'happens': 1, 'come': 1, 'back': 1, 'this': 1, 'spot': 1, 'hide': 1, 'brush': 1, 'Before': 1, 'falls': 1, 'asleep': 1, 'tells': 1, 'have': 1, 'many': 1, 'various': 1, 'colors': 1}\n"
     ]
    }
   ],
   "source": [
    "word_count = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_count:\n",
    "        word_count[word] = 0\n",
    "    word_count[word] += 1\n",
    "\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>men</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dressed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>tells</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>have</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>many</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>various</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>colors</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  count\n",
       "0        Two      1\n",
       "1        men      2\n",
       "2          ,     17\n",
       "3    dressed      1\n",
       "4         in      3\n",
       "..       ...    ...\n",
       "191    tells      1\n",
       "192     have      1\n",
       "193     many      1\n",
       "194  various      1\n",
       "195   colors      1\n",
       "\n",
       "[196 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_df = pd.DataFrame(list(word_count.items()), columns=['word','count'])\n",
    "\n",
    "word_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Two', 'men'), ('men', ','), (',', 'dressed'), ('dressed', 'in'), ('in', 'denim'), ('denim', 'jackets'), ('jackets', 'and'), ('and', 'trousers'), ('trousers', 'and'), ('and', 'wearing'), ('wearing', '``'), ('``', 'black'), ('black', ','), (',', 'shapeless'), ('shapeless', 'hats'), ('hats', ','), (',', \"''\"), (\"''\", 'walk'), ('walk', 'single-file'), ('single-file', 'down'), ('down', 'a'), ('a', 'path'), ('path', 'near'), ('near', 'the'), ('the', 'pool'), ('pool', '.'), ('.', 'Both'), ('Both', 'men'), ('men', 'carry'), ('carry', 'blanket'), ('blanket', 'rolls'), ('rolls', '—'), ('—', 'called'), ('called', 'bindles'), ('bindles', '—'), ('—', 'on'), ('on', 'their'), ('their', 'shoulders'), ('shoulders', '.'), ('.', 'The'), ('The', 'smaller'), ('smaller', ','), (',', 'wiry'), ('wiry', 'man'), ('man', 'is'), ('is', 'George'), ('George', 'Milton'), ('Milton', '.'), ('.', 'Behind'), ('Behind', 'him'), ('him', 'is'), ('is', 'Lennie'), ('Lennie', 'Small'), ('Small', ','), (',', 'a'), ('a', 'huge'), ('huge', 'man'), ('man', 'with'), ('with', 'large'), ('large', 'eyes'), ('eyes', 'and'), ('and', 'sloping'), ('sloping', 'shoulders'), ('shoulders', ','), (',', 'walking'), ('walking', 'at'), ('at', 'a'), ('a', 'gait'), ('gait', 'that'), ('that', 'makes'), ('makes', 'him'), ('him', 'resemble'), ('resemble', 'a'), ('a', 'huge'), ('huge', 'bear'), ('bear', '.'), ('.', 'When'), ('When', 'Lennie'), ('Lennie', 'drops'), ('drops', 'near'), ('near', 'the'), ('the', 'pool'), ('pool', \"'s\"), (\"'s\", 'edge'), ('edge', 'and'), ('and', 'begins'), ('begins', 'to'), ('to', 'drink'), ('drink', 'like'), ('like', 'a'), ('a', 'hungry'), ('hungry', 'animal'), ('animal', ','), (',', 'George'), ('George', 'cautions'), ('cautions', 'him'), ('him', 'that'), ('that', 'the'), ('the', 'water'), ('water', 'may'), ('may', 'not'), ('not', 'be'), ('be', 'good'), ('good', '.'), ('.', 'This'), ('This', 'advice'), ('advice', 'is'), ('is', 'necessary'), ('necessary', 'because'), ('because', 'Lennie'), ('Lennie', 'is'), ('is', 'retarded'), ('retarded', 'and'), ('and', 'does'), ('does', \"n't\"), (\"n't\", 'realize'), ('realize', 'the'), ('the', 'possible'), ('possible', 'dangers'), ('dangers', '.'), ('.', 'The'), ('The', 'two'), ('two', 'are'), ('are', 'on'), ('on', 'their'), ('their', 'way'), ('way', 'to'), ('to', 'a'), ('a', 'ranch'), ('ranch', 'where'), ('where', 'they'), ('they', 'can'), ('can', 'get'), ('get', 'temporary'), ('temporary', 'work'), ('work', ','), (',', 'and'), ('and', 'George'), ('George', 'warns'), ('warns', 'Lennie'), ('Lennie', 'not'), ('not', 'to'), ('to', 'say'), ('say', 'anything'), ('anything', 'when'), ('when', 'they'), ('they', 'arrive'), ('arrive', '.'), ('.', 'Because'), ('Because', 'Lennie'), ('Lennie', 'forgets'), ('forgets', 'things'), ('things', 'very'), ('very', 'quickly'), ('quickly', ','), (',', 'George'), ('George', 'must'), ('must', 'make'), ('make', 'him'), ('him', 'repeat'), ('repeat', 'even'), ('even', 'the'), ('the', 'simplest'), ('simplest', 'instructions'), ('instructions', '.'), ('.', 'Lennie'), ('Lennie', 'also'), ('also', 'likes'), ('likes', 'to'), ('to', 'pet'), ('pet', 'soft'), ('soft', 'things'), ('things', '.'), ('.', 'In'), ('In', 'his'), ('his', 'pocket'), ('pocket', ','), (',', 'he'), ('he', 'has'), ('has', 'a'), ('a', 'dead'), ('dead', 'mouse'), ('mouse', 'which'), ('which', 'George'), ('George', 'confiscates'), ('confiscates', 'and'), ('and', 'throws'), ('throws', 'into'), ('into', 'the'), ('the', 'weeds'), ('weeds', 'beyond'), ('beyond', 'the'), ('the', 'pond'), ('pond', '.'), ('.', 'Lennie'), ('Lennie', 'retrieves'), ('retrieves', 'the'), ('the', 'dead'), ('dead', 'mouse'), ('mouse', ','), (',', 'and'), ('and', 'George'), ('George', 'once'), ('once', 'again'), ('again', 'catches'), ('catches', 'him'), ('him', 'and'), ('and', 'gives'), ('gives', 'Lennie'), ('Lennie', 'a'), ('a', 'lecture'), ('lecture', 'about'), ('about', 'the'), ('the', 'trouble'), ('trouble', 'he'), ('he', 'causes'), ('causes', 'when'), ('when', 'he'), ('he', 'wants'), ('wants', 'to'), ('to', 'pet'), ('pet', 'soft'), ('soft', 'things'), ('things', '('), ('(', 'they'), ('they', 'were'), ('were', 'run'), ('run', 'out'), ('out', 'of'), ('of', 'the'), ('the', 'last'), ('last', 'town'), ('town', 'because'), ('because', 'Lennie'), ('Lennie', 'touched'), ('touched', 'a'), ('a', 'girl'), ('girl', \"'s\"), (\"'s\", 'soft'), ('soft', 'dress'), ('dress', ','), (',', 'and'), ('and', 'she'), ('she', 'screamed'), ('screamed', ')'), (')', '.'), ('.', 'Lennie'), ('Lennie', 'offers'), ('offers', 'to'), ('to', 'leave'), ('leave', 'and'), ('and', 'go'), ('go', 'live'), ('live', 'in'), ('in', 'a'), ('a', 'cave'), ('cave', ','), (',', 'causing'), ('causing', 'George'), ('George', 'to'), ('to', 'soften'), ('soften', 'his'), ('his', 'complaint'), ('complaint', 'and'), ('and', 'tell'), ('tell', 'Lennie'), ('Lennie', 'perhaps'), ('perhaps', 'they'), ('they', 'can'), ('can', 'get'), ('get', 'him'), ('him', 'a'), ('a', 'puppy'), ('puppy', 'that'), ('that', 'can'), ('can', 'withstand'), ('withstand', 'Lennie'), ('Lennie', \"'s\"), (\"'s\", 'petting'), ('petting', '.'), ('.', 'As'), ('As', 'they'), ('they', 'get'), ('get', 'ready'), ('ready', 'to'), ('to', 'eat'), ('eat', 'and'), ('and', 'sleep'), ('sleep', 'for'), ('for', 'the'), ('the', 'night'), ('night', ','), (',', 'Lennie'), ('Lennie', 'asks'), ('asks', 'George'), ('George', 'to'), ('to', 'repeat'), ('repeat', 'their'), ('their', 'dream'), ('dream', 'of'), ('of', 'having'), ('having', 'their'), ('their', 'own'), ('own', 'ranch'), ('ranch', 'where'), ('where', 'Lennie'), ('Lennie', 'will'), ('will', 'be'), ('be', 'able'), ('able', 'to'), ('to', 'tend'), ('tend', 'rabbits'), ('rabbits', '.'), ('.', 'George'), ('George', 'does'), ('does', 'so'), ('so', 'and'), ('and', 'then'), ('then', 'warns'), ('warns', 'Lennie'), ('Lennie', 'that'), ('that', ','), (',', 'if'), ('if', 'anything'), ('anything', 'bad'), ('bad', 'happens'), ('happens', ','), (',', 'Lennie'), ('Lennie', 'is'), ('is', 'to'), ('to', 'come'), ('come', 'back'), ('back', 'to'), ('to', 'this'), ('this', 'spot'), ('spot', 'and'), ('and', 'hide'), ('hide', 'in'), ('in', 'the'), ('the', 'brush'), ('brush', '.'), ('.', 'Before'), ('Before', 'George'), ('George', 'falls'), ('falls', 'asleep'), ('asleep', ','), (',', 'Lennie'), ('Lennie', 'tells'), ('tells', 'him'), ('him', 'they'), ('they', 'must'), ('must', 'have'), ('have', 'many'), ('many', 'rabbits'), ('rabbits', 'of'), ('of', 'various'), ('various', 'colors'), ('colors', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "bigrams_list = list(bigrams(words))\n",
    "\n",
    "print(bigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Two, men)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(men, ,)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(,, dressed)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(dressed, in)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(in, denim)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>(many, rabbits)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>(rabbits, of)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>(of, various)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>(various, colors)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>(colors, .)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>334 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                bigram  count\n",
       "0           (Two, men)      1\n",
       "1             (men, ,)      1\n",
       "2         (,, dressed)      1\n",
       "3        (dressed, in)      1\n",
       "4          (in, denim)      1\n",
       "..                 ...    ...\n",
       "329    (many, rabbits)      1\n",
       "330      (rabbits, of)      1\n",
       "331      (of, various)      1\n",
       "332  (various, colors)      1\n",
       "333        (colors, .)      1\n",
       "\n",
       "[334 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_count = {}\n",
    "\n",
    "for bigram in bigrams_list:\n",
    "    if bigram not in bigrams_count:\n",
    "        bigrams_count[bigram] = 0\n",
    "    bigrams_count[bigram] += 1\n",
    "\n",
    "bigrams_df = pd.DataFrame(list(bigrams_count.items()), columns = ['bigram', 'count'])\n",
    "\n",
    "bigrams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Two, men)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(men, ,)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(,, dressed)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(dressed, in)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(in, denim)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(denim, jackets)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(jackets, and)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(and, trousers)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(trousers, and)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(and, wearing)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(wearing, ``)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(``, black)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(black, ,)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(,, shapeless)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(shapeless, hats)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(hats, ,)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(,, '')</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>('', walk)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(walk, single-file)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(single-file, down)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(down, a)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(a, path)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(path, near)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(near, the)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(the, pool)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(pool, .)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(., Both)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(Both, men)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(men, carry)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(carry, blanket)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(blanket, rolls)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(rolls, —)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(—, called)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(called, bindles)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(bindles, —)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(—, on)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(on, their)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(their, shoulders)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(shoulders, .)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(., The)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(The, smaller)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(smaller, ,)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(,, wiry)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(wiry, man)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(man, is)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(is, George)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(George, Milton)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(Milton, .)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>(., Behind)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>(Behind, him)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bigram  count\n",
       "0            (Two, men)      1\n",
       "1              (men, ,)      1\n",
       "2          (,, dressed)      1\n",
       "3         (dressed, in)      1\n",
       "4           (in, denim)      1\n",
       "5      (denim, jackets)      1\n",
       "6        (jackets, and)      1\n",
       "7       (and, trousers)      1\n",
       "8       (trousers, and)      1\n",
       "9        (and, wearing)      1\n",
       "10        (wearing, ``)      1\n",
       "11          (``, black)      1\n",
       "12           (black, ,)      1\n",
       "13       (,, shapeless)      1\n",
       "14    (shapeless, hats)      1\n",
       "15            (hats, ,)      1\n",
       "16              (,, '')      1\n",
       "17           ('', walk)      1\n",
       "18  (walk, single-file)      1\n",
       "19  (single-file, down)      1\n",
       "20            (down, a)      1\n",
       "21            (a, path)      1\n",
       "22         (path, near)      1\n",
       "23          (near, the)      2\n",
       "24          (the, pool)      2\n",
       "25            (pool, .)      1\n",
       "26            (., Both)      1\n",
       "27          (Both, men)      1\n",
       "28         (men, carry)      1\n",
       "29     (carry, blanket)      1\n",
       "30     (blanket, rolls)      1\n",
       "31           (rolls, —)      1\n",
       "32          (—, called)      1\n",
       "33    (called, bindles)      1\n",
       "34         (bindles, —)      1\n",
       "35              (—, on)      1\n",
       "36          (on, their)      2\n",
       "37   (their, shoulders)      1\n",
       "38       (shoulders, .)      1\n",
       "39             (., The)      2\n",
       "40       (The, smaller)      1\n",
       "41         (smaller, ,)      1\n",
       "42            (,, wiry)      1\n",
       "43          (wiry, man)      1\n",
       "44            (man, is)      1\n",
       "45         (is, George)      1\n",
       "46     (George, Milton)      1\n",
       "47          (Milton, .)      1\n",
       "48          (., Behind)      1\n",
       "49        (Behind, him)      1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>(,, and)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>(,, Lennie)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>(., Lennie)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>(dead, mouse)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(the, pool)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>(n't, realize)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>(does, n't)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>(and, does)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>(retarded, and)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>(colors, .)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>334 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              bigram  count\n",
       "131         (,, and)      3\n",
       "274      (,, Lennie)      3\n",
       "159      (., Lennie)      3\n",
       "175    (dead, mouse)      2\n",
       "24       (the, pool)      2\n",
       "..               ...    ...\n",
       "112   (n't, realize)      1\n",
       "111      (does, n't)      1\n",
       "110      (and, does)      1\n",
       "109  (retarded, and)      1\n",
       "333      (colors, .)      1\n",
       "\n",
       "[334 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_df.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1 TFIDF: (better,0.354) (done,0.354) (ever,0.354) (far,0.707) (thing,0.354)\n",
      "Doc 2 TFIDF: (call,0.707) (ishmael,0.707)\n",
      "Doc 3 TFIDF: (dagger,0.447) (see,0.894)\n",
      "Doc 4 TFIDF: (dagger,0.333) (happi,0.667) (o,0.667)\n",
      "Doc 1 sim: [ 1.000 0.000 0.000 0.000 ]\n",
      "Doc 2 sim: [ 0.000 1.000 0.000 0.000 ]\n",
      "Doc 3 sim: [ 0.000 0.000 1.000 0.149 ]\n",
      "Doc 4 sim: [ 0.000 0.000 0.149 1.000 ]\n"
     ]
    }
   ],
   "source": [
    "# Convert term vectors into gensim dictionary\n",
    "\n",
    "import gensim\n",
    "\n",
    "term_vec = [\n",
    "    ['far', 'far', 'better', 'thing', 'ever', 'done'],\n",
    "    ['call', 'ishmael'],\n",
    "    ['dagger', 'see'],\n",
    "    ['o', 'happi', 'dagger']\n",
    "]\n",
    "\n",
    "dict = gensim.corpora.Dictionary( term_vec )\n",
    "\n",
    "corp = [ ]\n",
    "for i in range( 0, len( term_vec ) ):\n",
    "    corp.append( dict.doc2bow( term_vec[ i ] ) )\n",
    "\n",
    "#  Create TFIDF vectors based on term vectors bag-of-word corpora\n",
    "\n",
    "tfidf_model = gensim.models.TfidfModel( corp )\n",
    "\n",
    "tfidf = [ ]\n",
    "for i in range( 0, len( corp ) ):\n",
    "    tfidf.append( tfidf_model[ corp[ i ] ] )\n",
    "\n",
    "#  Create pairwise document similarity index\n",
    "\n",
    "n = len( dict )\n",
    "index = gensim.similarities.SparseMatrixSimilarity( tfidf_model[ corp ], num_features = n )\n",
    "\n",
    "#  Print TFIDF vectors and pairwise similarity per document\n",
    "\n",
    "for i in range( 0, len( tfidf ) ):\n",
    "    s = 'Doc ' + str( i + 1 ) + ' TFIDF:'\n",
    "    \n",
    "    for j in range( 0, len( tfidf[ i ] ) ):\n",
    "        s = s + ' (' + dict.get( tfidf[ i ][ j ][ 0 ] ) + ','\n",
    "        s = s + ( '%.3f' % tfidf[ i ][ j ][ 1 ] ) + ')'\n",
    "\n",
    "    print( s )\n",
    "\n",
    "for i in range( 0, len( corp ) ):\n",
    "    print( 'Doc', ( i + 1 ), 'sim: [ ', end='' )\n",
    "    \n",
    "    sim = index[ tfidf_model[ corp[ i ] ] ]\n",
    "    for j in range( 0, len( sim ) ):\n",
    "        print( '%.3f ' % sim[ j ], end='' )\n",
    "\n",
    "    print( ']' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1 TFIDF:  (thing,0.408) (better,0.408) (far,0.816)\n",
      "Doc 2 TFIDF: (ishmael,1.000)\n",
      "Doc 3 TFIDF: (dagger,1.000)\n",
      "Doc 4 TFIDF: (happi,0.785) (dagger,0.619)\n",
      "Doc 1 sim: [  1.000  0.000  0.000  0.000  ]\n",
      "Doc 2 sim: [  0.000  1.000  0.000  0.000  ]\n",
      "Doc 3 sim: [  0.000  0.000  1.000  0.619  ]\n",
      "Doc 4 sim: [  0.000  0.000  0.619  1.000  ]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = [\\\n",
    "  \"It is a far, far better thing I do, than I have every done before\",\\\n",
    "  \"Call me Ishmael\",\\\n",
    "  \"Is this a dagger I see before me?\",\\\n",
    "  \"O happy dagger\"\\\n",
    "]\n",
    "\n",
    "#  Remove punctuation\n",
    "\n",
    "punc = re.compile( '[%s]' % re.escape( string.punctuation ) )\n",
    "for i, doc in enumerate( text ):\n",
    "    text[ i ] = punc.sub( '', doc.lower() )\n",
    "\n",
    "#  TF-IDF vectorize documents w/sklearn, remove English stop words\n",
    "\n",
    "vect = TfidfVectorizer( stop_words='english' )\n",
    "xform = vect.fit_transform( text )\n",
    "\n",
    "#  Grab remaining terms (keys), stem, if different replace w/stem\n",
    "\n",
    "porter = nltk.stem.porter.PorterStemmer()\n",
    "for term in list( vect.vocabulary_.keys() ):\n",
    "    if term == porter.stem( term ):\n",
    "        continue\n",
    "\n",
    "    v = vect.vocabulary_[ term ]\n",
    "    del vect.vocabulary_[ term ]\n",
    "    vect.vocabulary_[ porter.stem( term ) ] = v\n",
    "\n",
    "#  Get final key/value lists\n",
    "\n",
    "key = list( vect.vocabulary_.keys() )\n",
    "val = list( vect.vocabulary_.values() )\n",
    "\n",
    "# Print out formatted TF-IDF scores per term per document\n",
    "\n",
    "row, col = xform.nonzero()\n",
    "\n",
    "cur_doc = 0\n",
    "s = 'Doc 1 TFIDF: '\n",
    "\n",
    "for i, c in enumerate( col ):\n",
    "    term = key[ val.index( c ) ]\n",
    "    tfidf = xform[ row[ i ], c ]\n",
    "    \n",
    "    if row[ i ] != cur_doc:        #  New document?\n",
    "        print( s )                 #  Print prev doc's terms/TFIDF weights\n",
    "        \n",
    "        cur_doc = row[ i ]         #  Record new doc's ID\n",
    "        s = 'Doc ' + str( cur_doc + 1 ) + ' TFIDF:'\n",
    "\n",
    "    s = s + ' (' + term + ','      #  Add current term/TFIDF pair\n",
    "    s = s + ( f'{tfidf:.03f}' + ')' )\n",
    "\n",
    "print( s )                         #  Print final doc's terms/TFIDF weights\n",
    "\n",
    "# Print document similarity matrix\n",
    "\n",
    "dense = xform.todense()\n",
    "\n",
    "for i in range( len( dense ) ):\n",
    "    s = 'Doc ' + str( i + 1 ) + ' sim: '\n",
    "    x = dense[ i ].tolist()[ 0 ]\n",
    "    \n",
    "    s = s + '[  '\n",
    "    for j in range( len( dense ) ):\n",
    "        y = dense[ j ].tolist()[ 0 ]\n",
    "        prod = numpy.multiply( x, y ).sum()\n",
    "        s = s + f'{prod:.03f}' + '  '\n",
    "    print( s + ']' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Probability Matrices with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "   juliet (28.13%);  dagger (22.75%);  romeo (19.99%);  hampshire (15.51%);  new (13.63%); \n",
      "Concept 1:\n",
      "   new (29.75%);  hampshire (25.04%);  dagger (15.24%);  romeo (15.14%);  juliet (14.84%); \n",
      "Concept 2:\n",
      "   romeo (26.75%);  dagger (25.34%);  new (18.02%);  hampshire (16.64%);  juliet (13.26%); \n",
      "Concept 3:\n",
      "   hampshire (24.08%);  juliet (22.70%);  new (19.50%);  dagger (17.29%);  romeo (16.43%); \n",
      "Concept 4:\n",
      "   dagger (22.53%);  hampshire (22.47%);  romeo (19.19%);  juliet (18.22%);  new (17.59%); \n",
      "\n",
      "Romeo and Juliet:\n",
      "  Concept 0, 72.77%\n",
      "Juliet, O happy dagger!:\n",
      "  Concept 0, 72.83%\n",
      "Romeo died by a dagger:\n",
      "  Concept 2, 72.83%\n",
      "'Live free or die', that's the New Hampshire motto:\n",
      "  Concept 1, 72.91%\n",
      "Did you know that New Hampshire is in New England?:\n",
      "  Concept 1, 79.72%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def display_doc_2_topic(doc_2_topic, collect):\n",
    "    for i in range(0, len(collect)):\n",
    "        topic_wt = list(doc_2_topic[i])\n",
    "        idx = topic_wt.index(max(topic_wt))\n",
    "        \n",
    "        print(collect[i] + \":\")\n",
    "        print(f\"  Concept {idx}, {topic_wt[ idx ] * 100.0:.02f}%\")\n",
    "\n",
    "def display_topics(model, feat_nm, top_word_n):\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print(f\"Concept {i}:\")\n",
    "        topic_len = sum(topic)\n",
    "        \n",
    "        term = \" \".join(\n",
    "            [\n",
    "                f\"{feat_nm[i]} ({topic[i] / topic_len * 100.0:.02f}%); \"\n",
    "                for i in topic.argsort()[: -top_word_n - 1 : -1]\n",
    "            ]\n",
    "        )\n",
    "        print(\"   \" + term)\n",
    "\n",
    "#  Mainline\n",
    "\n",
    "collection = [\n",
    "    \"Romeo and Juliet\",\n",
    "    \"Juliet, O happy dagger!\",\n",
    "    \"Romeo died by a dagger\",\n",
    "    \"'Live free or die', that's the New Hampshire motto\",\n",
    "    \"Did you know that New Hampshire is in New England?\",\n",
    "]\n",
    "\n",
    "feat_n = 10\n",
    "\n",
    "#  Raw term counts for LDA\n",
    "\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=feat_n, stop_words=\"english\"\n",
    ")\n",
    "tf = tf_vectorizer.fit_transform(collection)\n",
    "tf_feat_nm = tf_vectorizer.get_feature_names_out()\n",
    "\n",
    "topic_n = 5\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=topic_n,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "lda_topic = lda.fit(tf)\n",
    "doc_2_topic = lda.transform(tf)\n",
    "\n",
    "top_word_n = 10\n",
    "display_topics(lda, tf_feat_nm, top_word_n)\n",
    "\n",
    "print()\n",
    "display_doc_2_topic(doc_2_topic, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wolverine\n",
      "[-2.6735e+00  1.1196e+00 -3.3309e+00 -2.2347e+00  1.7461e+00  2.0495e+00\n",
      " -2.9675e+00 -1.0149e-01  2.0264e+00 -4.3162e-01  2.5480e+00 -1.1983e+00\n",
      " -1.7306e+00  3.9947e+00  7.5663e-01 -2.2848e+00  2.0478e+00 -1.6955e+00\n",
      " -5.8163e-01 -3.2606e+00  5.8465e-01  2.4206e+00 -1.4075e+00 -2.6414e+00\n",
      "  5.6743e-01  1.3794e+00 -9.5934e-01 -2.0380e+00  5.9991e-01  7.2090e-01\n",
      " -8.3317e-01 -2.8623e+00 -6.4841e-01 -3.3744e+00 -4.4572e-02 -3.9805e+00\n",
      "  1.0150e+00  1.6878e+00  1.7412e+00  9.1148e-01  2.0837e+00  3.0709e+00\n",
      "  1.7426e+00  2.1406e+00 -2.1008e+00 -7.8994e-02  3.1308e+00 -2.4934e+00\n",
      "  1.6135e+00 -6.5501e-01 -9.9574e-02  6.4580e+00  1.0512e+00 -1.0394e+00\n",
      " -3.4681e+00 -1.2427e+00 -9.3425e-01  1.6353e-01  2.9157e+00 -1.3834e+00\n",
      " -1.4721e+00 -2.3015e+00  7.9756e-03 -4.2218e-01 -9.7589e-01  2.0461e+00\n",
      " -2.5379e+00 -1.1676e+00  1.5319e+00 -2.5456e+00  4.1196e+00  1.3749e+00\n",
      "  5.3369e-01  9.1249e-01 -1.4115e-01 -1.8317e+00 -4.2549e+00 -3.0651e-01\n",
      " -1.1053e+00 -2.1728e+00 -1.9436e+00 -1.9733e+00 -2.8439e-01  6.6720e-01\n",
      "  4.4850e+00 -2.2496e+00 -2.3877e+00 -9.6623e-01  1.6513e+00  9.8415e-01\n",
      " -2.1489e-01 -3.1113e+00  3.7142e+00 -1.8501e+00  3.5076e-01  1.7822e-01\n",
      " -2.8218e-01 -6.6171e-01  1.8625e+00 -5.6834e-01  1.4724e+00 -3.9801e-01\n",
      "  2.2517e+00  1.4938e-01 -2.2513e-01  5.3020e-01 -1.3195e-01  3.3200e+00\n",
      " -5.0174e-01 -1.3751e+00 -2.5913e+00  1.5496e+00  3.3947e+00 -1.0691e+00\n",
      "  3.6809e+00  1.5029e+00 -1.1090e+00  1.7098e+00  2.8512e+00  9.8076e-01\n",
      " -1.1832e+00  1.0182e+00 -2.5982e+00 -2.1933e+00 -5.9633e+00 -2.9299e-01\n",
      "  1.8784e+00 -1.0084e+00 -5.9931e-01  2.4759e+00  1.1853e+00 -2.8067e+00\n",
      " -2.0392e-01  2.4474e+00  3.9705e-01  1.2345e+00 -1.2795e+00  1.6992e+00\n",
      " -7.8225e-01 -3.3117e+00 -3.8813e+00 -2.3910e+00 -3.9327e+00 -1.3294e+00\n",
      "  2.8075e-01  1.4538e+00 -2.5669e+00  2.6469e+00 -1.7957e+00  1.8491e+00\n",
      "  2.4852e+00  1.7321e+00 -2.6464e-02 -6.9149e-01 -1.9174e+00  2.3078e+00\n",
      "  2.6149e+00 -2.1952e-02  1.8926e+00  1.7076e+00 -5.0278e-01 -2.6523e+00\n",
      "  3.7090e-01  6.6376e-01 -2.4814e+00 -3.4606e-04 -2.6121e+00  1.5413e+00\n",
      " -1.2541e-02  2.5739e+00 -2.7109e+00  1.5907e+00  8.2901e-01  2.4093e+00\n",
      "  1.2778e+00 -4.4149e+00  2.1235e+00  3.4795e+00 -2.1072e+00 -1.8588e-01\n",
      " -2.5044e+00 -2.5650e+00  2.1292e+00  8.0867e-01  1.6559e+00  1.0846e+00\n",
      " -5.6096e-01 -4.9143e-01  1.0768e+00 -2.4742e+00  1.3853e+00  2.4557e+00\n",
      "  3.7934e-01 -1.7264e+00 -1.0347e+00  2.5347e-01 -3.5760e+00  3.1329e+00\n",
      " -7.9951e-01  1.2269e+00  3.1723e+00 -1.9662e-01  3.4254e-01 -1.5350e-01\n",
      " -5.7427e-01 -8.0735e-02  6.1703e-01 -2.5257e+00 -2.0012e-02 -5.6869e-01\n",
      "  1.0121e+00  3.9835e+00 -4.4168e+00  2.1621e+00 -5.6859e-01  1.9471e+00\n",
      " -4.4434e-01 -2.9292e-02 -1.3423e+00 -3.3957e+00  1.7311e+00  4.3647e-01\n",
      "  1.3100e+00  4.1315e+00 -1.1113e+00  3.1187e+00 -1.0356e+00  2.0044e+00\n",
      "  1.0079e+00  3.1546e-02  1.7999e-01 -3.2047e+00 -1.8070e-01  2.0230e+00\n",
      "  1.0664e+00  1.7031e+00  1.7713e+00  9.7692e-02  1.8810e+00  1.3176e+00\n",
      " -2.8733e+00 -1.4993e+00  9.5245e-01 -1.6645e-01 -1.8947e+00  2.4564e+00\n",
      "  3.8388e-01  3.1480e+00  2.7378e+00  3.3714e+00 -2.4354e-01 -1.2745e+00\n",
      " -2.8727e+00  1.1355e+00 -2.6493e+00  5.8062e-01 -1.6927e+00  5.8173e+00\n",
      "  2.2069e-01 -7.8541e-02  4.3284e+00 -1.4760e+00  2.3798e-01  6.7875e-01\n",
      " -2.0849e+00 -2.1036e+00 -1.8574e+00  5.9027e-01 -3.7323e+00  1.2919e+00\n",
      "  2.2779e+00 -3.0011e-01  1.6528e+00  1.2452e+00  1.2814e+00  3.6453e-01\n",
      " -2.1334e+00 -3.7065e+00 -4.9318e+00  2.6790e+00 -1.8958e+00 -2.8721e+00\n",
      "  1.2028e+00 -7.4647e-01  2.7768e+00  4.0181e-02 -2.7963e+00  2.2414e-02\n",
      " -5.5899e-01 -2.0377e-02  3.8004e-01 -1.6890e-01  4.4838e+00 -5.8246e-01\n",
      "  1.6391e-01  8.9963e-01 -1.5515e+00  3.9043e-01 -6.3748e+00 -5.8747e-01]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load( 'en_core_web_md' )\n",
    "doc = nlp( \"wolverine\" )\n",
    "\n",
    "print( doc[ 0 ].text )\n",
    "print( doc[ 0 ].vector )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Text Preprocessing. Prepare the text for NER using standard text preprocessing operations like tokenization and part-of-speech tagging.\n",
    "\n",
    "Entity Identification. Identify entities in the text.\n",
    "\n",
    "Entity Classification. Classify identified entities.\n",
    "\n",
    "Contextual Analysis. Use context to verify and correct entity classification, for example, does \"apple\" refer to a company or a fruit? Context may clarify this ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy:\n",
      "Deepak Jasani: PERSON; HDFC Securities: ORG; the European CentralBank: ORG; later Thursday: DATE; ECB: ORG; later Thursday: DATE; US: GPE; weekly: DATE; \n",
      "\n",
      "NLTK:\n",
      "Deepak: PERSON; Jasani: ORGANIZATION; HDFC: ORGANIZATION; European: ORGANIZATION; ECB: ORGANIZATION; US: GSP; \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "#  Specify text to NER on\n",
    "\n",
    "ex = \"\"\"Deepak Jasani, Head of retal research, HDFC Securities, said: \"Investors will look to the European CentralBank later Thursday for resassurance \n",
    "that surging prices are just transitory, and not about to spiral out of control. In addition to the ECB policy meeting, investors are awaiting a report \n",
    "later Thursday on US economic growth, which is likely to show a cooling recovery, as well as weekly jobs data.\".\"\"\"\n",
    "\n",
    "#  spaCy\n",
    "#  NLP process sentence, extract named entities from result\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(ex)\n",
    "\n",
    "print(\"spaCy:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_}; \", end=\"\")\n",
    "print( \"\\n\" )\n",
    "\n",
    "#  NLTK\n",
    "#  Tokenize and POS-tag tokens\n",
    "\n",
    "tok = nltk.tokenize.word_tokenize(ex)\n",
    "term_POS = nltk.tag.pos_tag(tok)\n",
    "\n",
    "#  Convert POS-tagged tokens into named entities\n",
    "\n",
    "NE_tree = nltk.ne_chunk(term_POS)\n",
    "\n",
    "print(\"NLTK:\")\n",
    "for ent in NE_tree:\n",
    "    if type(ent) == tuple:\n",
    "        continue\n",
    "    print(f\"{ent[0][0]}: {ent._label}; \", end=\"\")\n",
    "print( \"\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valence': 8.21, 'arousal': 6.49}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentiment_module import sentiment\n",
    "\n",
    "term = 'happy'\n",
    "sentiment.exist( term )\n",
    "sentiment.sentiment( term )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valence': 2.46, 'arousal': 7.97}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentiment_module import sentiment\n",
    "\n",
    "term = 'popsicle'\n",
    "sentiment.exist( term )\n",
    "term = 'enraged'\n",
    "sentiment.exist( term )\n",
    "sentiment.sentiment( term )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'valence': 5.0307617694606375, 'arousal': 4.939546556471719}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "term_list = \"it was the best of times it was the worst of times\".split()\n",
    "print(term_list)\n",
    "sentiment.exist( term_list )\n",
    "sentiment.sentiment( term_list )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very nervous'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "term_list = [ 'brocolli', 'carrot', 'pea' ]\n",
    "sentiment.exist( term_list )\n",
    "sentiment.sentiment( term_list )\n",
    "sentiment.describe( 'interesting' )\n",
    "sentiment.describe( 'pensive' )\n",
    "sentiment.describe( [ 'quick', 'brown', 'fox', 'jumps', 'lazy', 'dog' ] )\n",
    "sentiment.describe( 1, 9 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis at the Sentence Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two men, dressed in denim jackets and trousers and wearing \"black, shapeless hats,\" walk single-file down a path near the pool\n",
      "Sentiment: 0.0\n",
      " Both men carry blanket rolls  called bindles  on their shoulders\n",
      "Sentiment: 0.0\n",
      " The smaller, wiry man is George Milton\n",
      "Sentiment: 0.0\n",
      " Behind him is Lennie Small, a huge man with large eyes and sloping shoulders, walking at a gait that makes him resemble a huge bear\n",
      "Sentiment: 0.5574\n",
      " When Lennie drops near the pool's edge and begins to drink like a hungry animal, George cautions him that the water may not be good\n",
      "Sentiment: 0.0243\n",
      " This advice is necessary because Lennie is retarded and doesn't realize the possible dangers\n",
      "Sentiment: -0.7845\n",
      " The two are on their way to a ranch where they can get temporary work, and George warns Lennie not to say anything when they arrive\n",
      "Sentiment: -0.1027\n",
      " Because Lennie forgets things very quickly, George must make him repeat even the simplest instructions\n",
      "Sentiment: 0.0\n",
      " Lennie also likes to pet soft things\n",
      "Sentiment: 0.4215\n",
      " In his pocket, he has a dead mouse which George confiscates and throws into the weeds beyond the pond\n",
      "Sentiment: -0.6486\n",
      " Lennie retrieves the dead mouse, and George once again catches him and gives Lennie a lecture about the trouble he causes when he wants to pet soft things (they were run out of the last town because Lennie touched a girl's soft dress, and she screamed)\n",
      "Sentiment: -0.7906\n",
      " Lennie offers to leave and go live in a cave, causing George to soften his complaint and tell Lennie perhaps they can get him a puppy that can withstand Lennie's petting\n",
      "Sentiment: -0.34\n",
      " As they get ready to eat and sleep for the night, Lennie asks George to repeat their dream of having their own ranch where Lennie will be able to tend rabbits\n",
      "Sentiment: 0.5423\n",
      " George does so and then warns Lennie that, if anything bad happens, Lennie is to come back to this spot and hide in the brush\n",
      "Sentiment: -0.7063\n",
      " Before George falls asleep, Lennie tells him they must have many rabbits of various colors\n",
      "Sentiment: 0.0\n",
      "\n",
      "Sentiment: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/frederik.lindsey/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download( 'vader_lexicon' )\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "txt = 'Two men, dressed in denim jackets and trousers and wearing \"black, shapeless hats,\" walk single-file down a path near the pool. Both men carry blanket rolls  called bindles  on their shoulders. The smaller, wiry man is George Milton. Behind him is Lennie Small, a huge man with large eyes and sloping shoulders, walking at a gait that makes him resemble a huge bear. When Lennie drops near the pool\\'s edge and begins to drink like a hungry animal, George cautions him that the water may not be good. This advice is necessary because Lennie is retarded and doesn\\'t realize the possible dangers. The two are on their way to a ranch where they can get temporary work, and George warns Lennie not to say anything when they arrive. Because Lennie forgets things very quickly, George must make him repeat even the simplest instructions. Lennie also likes to pet soft things. In his pocket, he has a dead mouse which George confiscates and throws into the weeds beyond the pond. Lennie retrieves the dead mouse, and George once again catches him and gives Lennie a lecture about the trouble he causes when he wants to pet soft things (they were run out of the last town because Lennie touched a girl\\'s soft dress, and she screamed). Lennie offers to leave and go live in a cave, causing George to soften his complaint and tell Lennie perhaps they can get him a puppy that can withstand Lennie\\'s petting. As they get ready to eat and sleep for the night, Lennie asks George to repeat their dream of having their own ranch where Lennie will be able to tend rabbits. George does so and then warns Lennie that, if anything bad happens, Lennie is to come back to this spot and hide in the brush. Before George falls asleep, Lennie tells him they must have many rabbits of various colors.'\n",
    "\n",
    "#  Convert to sentences, create VADER sentiment analyzer\n",
    "\n",
    "sentence = txt.split( '.' )\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "\n",
    "for i in range( 0, len( sentence ) ):\n",
    "\n",
    "    # Print sentence's compound sentiment score\n",
    "    \n",
    "    score = sentiment.polarity_scores( sentence[ i ] )\n",
    "    print( sentence[ i ] )\n",
    "    print( 'Sentiment:', score[ 'compound' ] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
